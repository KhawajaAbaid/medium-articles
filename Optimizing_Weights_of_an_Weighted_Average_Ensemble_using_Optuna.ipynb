{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "EKw5IdHk-FlC",
        "Cw66xK8QLXdS",
        "0srzEzYoLfUW",
        "MerCutelLnM1",
        "so4DZ_QWLxDr",
        "Vu7u7dzkLrdS",
        "gwKwh8xdg0sq"
      ],
      "authorship_tag": "ABX9TyMtbtkIgH5TNIm84nlnA3ka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhawajaAbaid/medium-articles/blob/main/Optimizing_Weights_of_an_Weighted_Average_Ensemble_using_Optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction:\n",
        "Taking weighted average can result in imporved score but the problem arises when deciding what weight to use for which model. Even if we do manage to choose some weights that improve our score in comparison to the simple average, how do we know these are the optimal weights? This is where this article comes in, using the very easy, effective and fast hyperparameter framework like optuna to find the optimal values for our weights, that result in the best score possible.\n",
        "\n",
        "\n",
        "In this notebook, we'll introduce a new technique of finding optimized weights for combining multiple models into an ensemble using a popular hyperparameters tuning library called Optuna.\n",
        "\n",
        "Dataset: California Housing Price\n",
        "\n",
        "Task: Regression\n",
        "\n",
        "Models: Ridge, XGBoost, LightGBM"
      ],
      "metadata": {
        "id": "fKS6gZ1P67zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing and Importing required libraries â€” for our tutorial."
      ],
      "metadata": {
        "id": "EKw5IdHk-FlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "u7-DiG8w-mDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GTonFnf36pZ_"
      },
      "outputs": [],
      "source": [
        "# The essential libraries to work with data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine learning models that we'll use\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgbm\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "# optuna\n",
        "import optuna\n",
        "\n",
        "# We'll use MSE as our metric\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fetching the data"
      ],
      "metadata": {
        "id": "Cw66xK8QLXdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We set as_frame parameter to True and access the return object's \"frame\"\n",
        "# attribute to get the dataset as pandas dataframe.\n",
        "\n",
        "df = fetch_california_housing(as_frame=True)[\"frame\"]\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oRwYX3Mw-D4K",
        "outputId": "7e5455aa-26db-4704-a3fc-585c7876d3f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
              "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
              "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
              "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
              "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
              "\n",
              "   Longitude  MedHouseVal  \n",
              "0    -122.23        4.526  \n",
              "1    -122.22        3.585  \n",
              "2    -122.24        3.521  \n",
              "3    -122.25        3.413  \n",
              "4    -122.25        3.422  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-364b9bd3-3253-4ea5-b45a-ee1c7d275d8e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedHouseVal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.3252</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.984127</td>\n",
              "      <td>1.023810</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "      <td>4.526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.3014</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.238137</td>\n",
              "      <td>0.971880</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>2.109842</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "      <td>3.585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.2574</td>\n",
              "      <td>52.0</td>\n",
              "      <td>8.288136</td>\n",
              "      <td>1.073446</td>\n",
              "      <td>496.0</td>\n",
              "      <td>2.802260</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.24</td>\n",
              "      <td>3.521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.6431</td>\n",
              "      <td>52.0</td>\n",
              "      <td>5.817352</td>\n",
              "      <td>1.073059</td>\n",
              "      <td>558.0</td>\n",
              "      <td>2.547945</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>3.413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.8462</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.281853</td>\n",
              "      <td>1.081081</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.181467</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "      <td>3.422</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-364b9bd3-3253-4ea5-b45a-ee1c7d275d8e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-364b9bd3-3253-4ea5-b45a-ee1c7d275d8e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-364b9bd3-3253-4ea5-b45a-ee1c7d275d8e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ivRIEIwbwpo",
        "outputId": "72fbf5af-2044-4a3c-fc1c-7d6c1da9fa02"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20640"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Preprocessing"
      ],
      "metadata": {
        "id": "0srzEzYoLfUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MedHouseVal is our target variable â€” the variable that we'll predict based on the rest of the columns/features.\n",
        "\n",
        "But first, we need to check for any missing values in the dataset and fill them."
      ],
      "metadata": {
        "id": "ZHczUq1o_T3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wii9l6zv_9ZB",
        "outputId": "67414779-5278-463d-a94b-f747e635cc19"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MedInc         0\n",
              "HouseAge       0\n",
              "AveRooms       0\n",
              "AveBedrms      0\n",
              "Population     0\n",
              "AveOccup       0\n",
              "Latitude       0\n",
              "Longitude      0\n",
              "MedHouseVal    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well looks like we're lucky to have no missing values. So get our data ready for training."
      ],
      "metadata": {
        "id": "deNEC09WAZ3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's store target values in a separete variable. And then drop the target column from our dataset."
      ],
      "metadata": {
        "id": "jqLuuLbqcGwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = df.MedHouseVal\n",
        "df = df.drop(columns=[\"MedHouseVal\"])"
      ],
      "metadata": {
        "id": "tcm87O0PcYLP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split our dataset into a train and validation set. For that we'll use scikit-learn's train_test_split method."
      ],
      "metadata": {
        "id": "5IRIwL8zbdp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# we'll use a validation size of 20% or 0.20\n",
        "X_train, X_val, y_train, y_val = train_test_split(df, y,\n",
        "                                                  test_size=0.2,\n",
        "                                                  shuffle=True,\n",
        "                                                  random_state=1337)"
      ],
      "metadata": {
        "id": "kixjyq4DboJx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting a Baseline"
      ],
      "metadata": {
        "id": "MerCutelLnM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we optimize the weights, let's first set a baseline by taking simple average over all model's predicitons."
      ],
      "metadata": {
        "id": "vzWMoZ8Ce3Ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating the model\n",
        "ridge_model = Ridge()\n",
        "# training the model\n",
        "ridge_model.fit(X_train, y_train)\n",
        "# predicting using the trained model\n",
        "y_pred_ridge = ridge_model.predict(X_val)\n",
        "\n",
        "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
        "xgb_model.fit(X_train, y_train, verbose=False)\n",
        "y_pred_xgb = xgb_model.predict(X_val)\n",
        "\n",
        "lgbm_model = lgbm.LGBMRegressor()\n",
        "lgbm_model.fit(X_train, y_train, verbose=-1)\n",
        "y_pred_lgbm = lgbm_model.predict(X_val)\n",
        "\n",
        "# combining predictions by taking simple average using numpy\n",
        "y_pred_final = np.mean([y_pred_ridge, y_pred_xgb, y_pred_lgbm], axis=0)\n",
        "\n",
        "# let's calculate mse\n",
        "mse = mean_squared_error(y_val, y_pred_final)\n",
        "\n",
        "print(f\"Simple Average Ensemble's MSE: {mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKGeVuSUfFmh",
        "outputId": "44061642-701f-4f86-af24-a87d7c394679"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple Average Ensemble's MSE: 0.26770854520705767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A gentle introduction to Optuna:\n",
        "Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. By default it uses a Bayesian optimizing algorithm (TPE) which is way faster than using somehting like GridSearchCV which checks for each set of hyperparameters values defining the search space. The reason behind why Bayesian optimization is faster than grid search is that it uses information from the previous iterations of search to choose future values instead of treating each set of hyperparameters independently.\n",
        "\n",
        "Due to optuna's high modularity, we can use it for our task of finding optimal weights for taking a weighted average, and much more.\n"
      ],
      "metadata": {
        "id": "dpPOFge0A7bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How Optuna works?\n",
        "Optuna requries us to define an objective function, which\n",
        "defines the values ranges for each hyperparameter that we wish to tune aka\n",
        "optimize\n",
        "trains the model using hyperparameters values that it chooses from that range predicts the values for the validation set\n",
        "computes the metric/score for those predictions\n",
        "and finally returns a score or metric that depending on the metric, we intend to maximize or minimize.\n",
        "\n",
        "We then create a new study, passing it direction parameter, call its optimize function passing it the objective function and number of trials we wish to run. Number of trials is how many times the study object will run the objective function, get the score value. Then depending on the return value and on the direction value (maximize or minimize), it will choose the next set of values for hyperparamters that it infers will achieve a better score."
      ],
      "metadata": {
        "id": "so4DZ_QWLxDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay enough with the theory, let's get our hands dirty with implementation!"
      ],
      "metadata": {
        "id": "Pq2VTjibL0Qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing weights with Optuna"
      ],
      "metadata": {
        "id": "Vu7u7dzkLrdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "  STEP_SIZE = 10\n",
        "\n",
        "  weights = []\n",
        "  all_models_predictions = []\n",
        "\n",
        "  # we'll use a variable for setting upper limit for suggested value\n",
        "  # since we intend to update it after each weight suggestion\n",
        "  upper_limit = 100\n",
        "\n",
        "  w_ridge = trial.suggest_int(\"w_ridge\", 0, upper_limit, step=STEP_SIZE)\n",
        "  weights.append(w_ridge)\n",
        "\n",
        "  # Update upper limit to 100 - all the previous weights combined, which in this case is just w_ridge\n",
        "  # WHY? well because we want to keep our sum of all weights equal to 100\n",
        "  # and this is one way of ensuring that!\n",
        "  upper_limit -= sum(weights)\n",
        "  upper_limit = upper_limit\n",
        "\n",
        "  w_xgb = trial.suggest_int(\"w_xgb\", 0, upper_limit, step=STEP_SIZE)\n",
        "  weights.append(w_xgb)\n",
        "\n",
        "  # for the final weight we won't use optuna, rather we'll manually set it equal\n",
        "  # to whatever value remains after subtracting the sum of suggested weight values from 100\n",
        "  # This will also make sure that the sum of all weights remains equal to 100.\n",
        "  w_lgbm = 100 - sum(weights)\n",
        "  weights.append(w_lgbm)\n",
        "\n",
        "  # Just as a sanity check, we'll check that the sum of all weights is equal to 100\n",
        "  weights_sum = sum(weights)\n",
        "\n",
        "  if weights_sum != 100:\n",
        "    raise Exception(f\"Weights sum must be equal to 100. Instead {weights_sum} was encountered!\")\n",
        "\n",
        "  \n",
        "  # We'll use the default parameter values for all our models\n",
        "  \n",
        "  ridge_model = Ridge()\n",
        "  ridge_model.fit(X_train, y_train)\n",
        "  y_pred_ridge = ridge_model.predict(X_val)\n",
        "  all_models_predictions.append(y_pred_ridge)\n",
        "\n",
        "  xgb_model = xgb.XGBRegressor(objective ='reg:squarederror')\n",
        "  xgb_model.fit(X_train, y_train, verbose=False)\n",
        "  y_pred_xgb = xgb_model.predict(X_val)\n",
        "  all_models_predictions.append(y_pred_xgb)\n",
        "\n",
        "  lgbm_model = lgbm.LGBMRegressor()\n",
        "  lgbm_model.fit(X_train, y_train, verbose=-1)\n",
        "  y_pred_lgbm = lgbm_model.predict(X_val)\n",
        "  all_models_predictions.append(y_pred_lgbm)\n",
        "\n",
        "\n",
        "  # let's take the weighted average of the predictions using numpy\n",
        "  y_pred_final = np.average(all_models_predictions, weights=weights, axis=0)\n",
        "\n",
        "  # computing our metric i.e. MSE\n",
        "  mse = mean_squared_error(y_val, y_pred_final)\n",
        "\n",
        "  return mse"
      ],
      "metadata": {
        "id": "I12LHtMkMDsV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case you're wondering why are we passing the same variable name as argument to suggest_int function, it is because it's a way of telling optuna to store the parameter value against that name â€” basically using it as a key, and storing that key value pair inside its memory. For instance, when you'd access your optimized values after the study has completed, optuna will return a dictionary of key value pairs, with keys being the names you specified in the suggest function argument, and values being the optimized values of hyperparameters.\n",
        "\n",
        "It is important to remember that all the names you pass to functions should be unique!"
      ],
      "metadata": {
        "id": "pzeQysMOW53g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(study_name=\"optimizing weights\", direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POxNt0evB0CM",
        "outputId": "5cb814c1-9061-43eb-e090-b0d5f96ee9fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-01-19 17:18:56,884]\u001b[0m A new study created in memory with name: optimizing weights\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:18:58,344]\u001b[0m Trial 0 finished with value: 0.2646131415415177 and parameters: {'w_ridge': 40, 'w_xgb': 10}. Best is trial 0 with value: 0.2646131415415177.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:18:59,778]\u001b[0m Trial 1 finished with value: 0.20799930576386458 and parameters: {'w_ridge': 0, 'w_xgb': 10}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:01,198]\u001b[0m Trial 2 finished with value: 0.3028275602304653 and parameters: {'w_ridge': 50, 'w_xgb': 20}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:02,629]\u001b[0m Trial 3 finished with value: 0.5081829880318993 and parameters: {'w_ridge': 100, 'w_xgb': 0}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:04,050]\u001b[0m Trial 4 finished with value: 0.2635175242461122 and parameters: {'w_ridge': 20, 'w_xgb': 60}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:05,522]\u001b[0m Trial 5 finished with value: 0.2860543198731729 and parameters: {'w_ridge': 20, 'w_xgb': 80}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:06,962]\u001b[0m Trial 6 finished with value: 0.2488089868451061 and parameters: {'w_ridge': 30, 'w_xgb': 20}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:08,414]\u001b[0m Trial 7 finished with value: 0.27732780891278025 and parameters: {'w_ridge': 10, 'w_xgb': 90}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:09,860]\u001b[0m Trial 8 finished with value: 0.36784654388765337 and parameters: {'w_ridge': 70, 'w_xgb': 10}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:11,292]\u001b[0m Trial 9 finished with value: 0.3632705249579617 and parameters: {'w_ridge': 60, 'w_xgb': 40}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:12,757]\u001b[0m Trial 10 finished with value: 0.22058208757381859 and parameters: {'w_ridge': 0, 'w_xgb': 40}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:14,231]\u001b[0m Trial 11 finished with value: 0.22058208757381859 and parameters: {'w_ridge': 0, 'w_xgb': 40}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:15,710]\u001b[0m Trial 12 finished with value: 0.23398628709211808 and parameters: {'w_ridge': 0, 'w_xgb': 60}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:17,162]\u001b[0m Trial 13 finished with value: 0.2153846913081679 and parameters: {'w_ridge': 0, 'w_xgb': 30}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:18,614]\u001b[0m Trial 14 finished with value: 0.45190023851456096 and parameters: {'w_ridge': 90, 'w_xgb': 0}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:20,082]\u001b[0m Trial 15 finished with value: 0.23723584827301622 and parameters: {'w_ridge': 20, 'w_xgb': 30}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:21,555]\u001b[0m Trial 16 finished with value: 0.27292431994193356 and parameters: {'w_ridge': 40, 'w_xgb': 20}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:23,024]\u001b[0m Trial 17 finished with value: 0.24585795207326297 and parameters: {'w_ridge': 10, 'w_xgb': 60}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:24,456]\u001b[0m Trial 18 finished with value: 0.25684328754299696 and parameters: {'w_ridge': 30, 'w_xgb': 30}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n",
            "\u001b[32m[I 2023-01-19 17:19:25,896]\u001b[0m Trial 19 finished with value: 0.40140539618892696 and parameters: {'w_ridge': 80, 'w_xgb': 0}. Best is trial 1 with value: 0.20799930576386458.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_value"
      ],
      "metadata": {
        "id": "TNMw8fUkVCfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e779dba-30b1-44e3-9da1-24d8dd512e28"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21000290127721108"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "id": "QO7Jv4vRc2lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our optimized weighted average has brought the mean square error to 0.21 in comparison to 0.26 which we got from taking simple average of the predictions of the same models.\n",
        "\n",
        "So the advantage of optimizing weights for taking a weighted average looks pretty clear.\n"
      ],
      "metadata": {
        "id": "yzB9qtp0gaTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Takeaways:\n",
        "* Taking weighted average can significantly imporve our score when compared to the simple average, as it did in our case.\n",
        "* Leveraging the high modularity of optuna, we can find optimal weights for our ensemble, very quickly and effectively.\n",
        "\n",
        "Hope you found this article useful! You can find the notebook with all the code and comments on Github, as well as Colab. Since this is my first article, so I'd welcome any feedback as it would help me improve and produce better content. You can reach me on Twitter on in comments."
      ],
      "metadata": {
        "id": "gwKwh8xdg0sq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Vp5PUsKLMiw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}